@phdthesis{garyfallidis2012towards,
    author = {Garyfallidis, E.},
    citeulike-article-id = {10502439},
    keywords = {similarity},
    posted-at = {2012-03-27 14:59:27},
    priority = {2},
    school = {University of Cambridge},
    title = {{Towards an accurate brain tractography}},
    year = {2012}
}

@inproceedings{dubuisson1994modified,
    abstract = {{The purpose of object matching is to decide the similarity between
two objects. This paper introduces 24 possible distance measures based
on the Hausdorff distance between two point sets. These measures can be
used to match two sets of edge points extracted from any two objects.
Based on experiments on synthetic images containing various levels of
noise, the authors determined that one of these distance measures,
called the modified Hausdorff distance (MHD) has the best performance
for object matching. The advantages of MHD ever other distances are also
demonstrated on several edge snaps of objects extracted from real images}},
    author = {Dubuisson, M. P. and Jain, A. K.},
    booktitle = {Proceedings of 12th International Conference on Pattern Recognition},
    citeulike-article-id = {1146653},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/ICPR.1994.576361},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=576361},
    doi = {10.1109/ICPR.1994.576361},
    isbn = {0-8186-6265-4},
    journal = {Pattern Recognition, 1994. Vol. 1 - Conference A: Computer Vision \& Image Processing., Proceedings of the 12th IAPR International Conference on},
    keywords = {distance, hausdorff, matching, object, recognition, similarity},
    location = {Jerusalem, Israel},
    pages = {566--568},
    posted-at = {2012-03-26 15:10:08},
    priority = {2},
    publisher = {IEEE Comput. Soc. Press},
    title = {{A modified Hausdorff distance for object matching}},
    url = {http://dx.doi.org/10.1109/ICPR.1994.576361},
    volume = {1},
    year = {1994}
}

@article{wang2011tractography,
    abstract = {{In this paper, we propose a new nonparametric Bayesian framework to cluster white matter fiber tracts into bundles using a hierarchical Dirichlet processes mixture (HDPM) model. The number of clusters is automatically learned driven by data with a Dirichlet process (DP) prior instead of being manually specified. After the models of bundles have been learned from training data without supervision, they can be used as priors to cluster/classify fibers of new subjects for comparison across subjects. When clustering fibers of new subjects, new clusters can be created for structures not observed in the training data. Our approach does not require computing pairwise distances between fibers and can cluster a huge set of fibers across multiple subjects. We present results on several data sets, the largest of which has more than 120,000 fibers.}},
    author = {Wang, Xiaogang and Grimson and Westin, Carl-Fredrik},
    citeulike-article-id = {7606199},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.neuroimage.2010.07.050},
    day = {01},
    doi = {10.1016/j.neuroimage.2010.07.050},
    issn = {10538119},
    journal = {NeuroImage},
    keywords = {simbad2011, similarity, univr2011},
    month = jan,
    number = {1},
    pages = {290--302},
    posted-at = {2012-03-26 14:57:09},
    priority = {2},
    title = {{Tractography segmentation using a hierarchical Dirichlet processes mixture model}},
    url = {http://dx.doi.org/10.1016/j.neuroimage.2010.07.050},
    volume = {54},
    year = {2011}
}

@article{juntu2010machine,
    abstract = {{
                To study, from a machine learning perspective, the performance of several machine learning classifiers that use texture analysis features extracted from soft-tissue tumors in nonenhanced T1-MRI images to discriminate between malignant and benign tumors.
                Texture analysis features were extracted from the tumor regions from T1-MRI images of clinically proven cases of 49 malignant and 86 benign soft-tissue tumors. Three conventional machine learning classifiers were trained and tested. The best classifier was compared to the radiologists by means of the McNemar's statistical test.
                The SVM classifier performs better than the neural network and the C4.5 decision tree based on the analysis of their receiver operating curves (ROC) and cost curves. The classification accuracy of the SVM, which was 93\% (91\% specificity; 94\% sensitivity), was better than the radiologist classification accuracy of 90\% (92\% specificity; 81\% sensitivity).
                Machine learning classifiers trained with texture analysis features are potentially valuable for detecting malignant tumors in T1-MRI images. Analysis of the learning curves of the classifiers showed that a training data size smaller than 100 T1-MRI images is sufficient to train a machine learning classifier that performs as well as expert radiologists.
            }},
    author = {Juntu, Jaber and Sijbers, Jan and De Backer, Steve and Rajan, Jeny and Van Dyck, Dirk},
    citeulike-article-id = {8828047},
    citeulike-linkout-0 = {http://dx.doi.org/10.1002/jmri.22095},
    citeulike-linkout-1 = {http://view.ncbi.nlm.nih.gov/pubmed/20187212},
    citeulike-linkout-2 = {http://www.hubmed.org/display.cgi?uids=20187212},
    doi = {10.1002/jmri.22095},
    issn = {1522-2586},
    journal = {Journal of magnetic resonance imaging : JMRI},
    keywords = {similarity},
    month = mar,
    number = {3},
    pages = {680--689},
    pmid = {20187212},
    posted-at = {2012-03-20 17:18:22},
    priority = {2},
    publisher = {Wiley Subscription Services, Inc., A Wiley Company},
    title = {{Machine learning study of several classifiers trained with texture analysis features to differentiate benign from malignant soft-tissue tumors in T1-MRI images.}},
    url = {http://dx.doi.org/10.1002/jmri.22095},
    volume = {31},
    year = {2010}
}

@article{zacharaki2011investigating,
    abstract = {{
                Diagnosis and characterization of brain neoplasms appears of utmost importance for therapeutic management. The emerging of imaging techniques, such as Magnetic Resonance (MR) imaging, gives insight into pathology, while the combination of several sequences from conventional and advanced protocols (such as perfusion imaging) increases the diagnostic information. To optimally combine the multiple sources and summarize the information into a distinctive set of variables however remains difficult. The purpose of this study is to investigate machine learning algorithms that automatically identify the relevant attributes and are optimal for brain tumor differentiation.
                Different machine learning techniques are studied for brain tumor classification based on attributes extracted from conventional and perfusion MRI. The attributes, calculated from neoplastic, necrotic, and edematous regions of interest, include shape and intensity characteristics. Attributes subset selection is performed aiming to remove redundant attributes using two filtering methods and a wrapper approach, in combination with three different search algorithms (Best First, Greedy Stepwise and Scatter). The classification frameworks are implemented using the WEKA software.
                The highest average classification accuracy assessed by leave-one-out (LOO) cross-validation on 101 brain neoplasms was achieved using the wrapper evaluator in combination with the Best First search algorithm and the KNN classifier and reached 96.9\% when discriminating metastases from gliomas and 94.5\% when discriminating high-grade from low-grade neoplasms.
                A computer-assisted classification framework is developed and used for differential diagnosis of brain neoplasms based on MRI. The framework can achieve higher accuracy than most reported studies using MRI.
            }},
    author = {Zacharaki, Evangelia I. and Kanas, Vasileios G. and Davatzikos, Christos},
    citeulike-article-id = {9236057},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/s11548-011-0559-3},
    citeulike-linkout-1 = {http://view.ncbi.nlm.nih.gov/pubmed/21516321},
    citeulike-linkout-2 = {http://www.hubmed.org/display.cgi?uids=21516321},
    citeulike-linkout-3 = {http://www.springerlink.com/content/71506u004415787m},
    day = {22},
    doi = {10.1007/s11548-011-0559-3},
    issn = {1861-6429},
    journal = {International journal of computer assisted radiology and surgery},
    keywords = {similarity},
    month = nov,
    number = {6},
    pages = {821--828},
    pmid = {21516321},
    posted-at = {2012-03-20 17:15:07},
    priority = {2},
    publisher = {Springer Berlin / Heidelberg},
    title = {{Investigating machine learning techniques for MRI-based classification of brain neoplasms.}},
    url = {http://dx.doi.org/10.1007/s11548-011-0559-3},
    volume = {6},
    year = {2011}
}

@article{zhang2008identifying,
    abstract = {{We present a method for clustering diffusion tensor imaging (DTI) integral curves into anatomically plausible bundles. An expert rater evaluated the anatomical accuracy of the bundles. We also evaluated the method by applying an experimental cross-subject labeling method to the clustering results. We first employ a sampling and culling strategy for generating DTI integral curves and then constrain the curves so that they terminate in gray matter. We then employ a clustering method based on a proximity measure calculated between every pair of curves. We interactively selected a proximity threshold to achieve visually optimal clustering in models from four DTI datasets. An expert rater then assigned a confidence rating about bundle presence and accuracy for each of 12 target fiber bundles of varying calibers and type in each dataset. We then created a fiber bundle template to cluster and label the fiber bundles automatically in new datasets. According to expert evaluation, the automated proximity-based clustering and labeling algorithm consistently yields anatomically plausible fiber bundles on large and coherent clusters. This work has the potential to provide an automatic and robust way to find and study neural fiber bundles within DTI.}},
    author = {Zhang, Song and Correia, S. and Laidlaw, D. H.},
    booktitle = {Visualization and Computer Graphics, IEEE Transactions on},
    citeulike-article-id = {4216076},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/TVCG.2008.52},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4479455},
    doi = {10.1109/TVCG.2008.52},
    journal = {Visualization and Computer Graphics, IEEE Transactions on},
    keywords = {similarity},
    number = {5},
    pages = {1044--1053},
    posted-at = {2012-03-20 16:53:31},
    priority = {2},
    title = {{Identifying White-Matter Fiber Bundles in DTI Data Using an Automated Proximity-Based Fiber-Clustering Method}},
    url = {http://dx.doi.org/10.1109/TVCG.2008.52},
    volume = {14},
    year = {2008}
}

@inproceedings{olivetti2011supervised,
    abstract = {{In this work we study the problem of supervised tract segmentation from tractography data, a vectorial representation of the brain connectivity extracted from diffusion magnetic resonance images. We report a case study based on a dataset where for each tractography of three subjects the segmentation of eight major anatomical tracts was manually operated by expert neuroanatomists. Domain specific distances that encodes the dissimilarity of tracts do not allow to define a positive semi-definite kernel function.We show that a dissimilarity representation based on such distances enables the successful design of a classifier. This approach provides a robust encoding which proves to be effective using a linear classifier. Our empirical analysis shows that we obtain better tract segmentation than previously proposed methods.}},
    address = {Berlin, Heidelberg},
    author = {Olivetti, Emanuele and Avesani, Paolo},
    booktitle = {Proceedings of the First international conference on Similarity-based pattern recognition},
    citeulike-article-id = {10372321},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/978-3-642-24471-1\_19},
    citeulike-linkout-1 = {http://portal.acm.org/citation.cfm?id=2046037},
    doi = {10.1007/978-3-642-24471-1\_19},
    isbn = {978-3-642-24470-4},
    keywords = {similarity},
    location = {Venice, Italy},
    pages = {261--274},
    posted-at = {2012-03-19 17:20:37},
    priority = {2},
    publisher = {Springer-Verlag},
    series = {SIMBAD'11},
    title = {{Supervised segmentation of fiber tracts}},
    url = {http://dx.doi.org/10.1007/978-3-642-24471-1\_19},
    year = {2011}
}

@article{tuch2002high,
    abstract = {{Magnetic resonance (MR) diffusion tensor imaging (DTI) can resolve the white matter fiber orientation within a voxel provided that the fibers are strongly aligned. However, a given voxel may contain a distribution of fiber orientations due to, for example, intravoxel fiber crossing. The present study sought to test whether a geodesic, high b-value diffusion gradient sampling scheme could resolve multiple fiber orientations within a single voxel. In regions of fiber crossing the diffusion signal exhibited multiple local maxima/minima as a function of diffusion gradient orientation, indicating the presence of multiple intravoxel fiber orientations. The multimodality of the observed diffusion signal precluded the standard tensor reconstruction, so instead the diffusion signal was modeled as arising from a discrete mixture of Gaussian diffusion processes in slow exchange, and the underlying mixture of tensors was solved for using a gradient descent scheme. The multitensor reconstruction resolved multiple intravoxel fiber populations corresponding to known fiber anatomy. Magn Reson Med 48:577–582, 2002. {\copyright} 2002 Wiley-Liss, Inc.}},
    address = {Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital, Charlestown, Massachusetts; Center for Morphometric Analysis, Massachusetts General Hospital, Charlestown, Massachusetts},
    author = {Tuch, David S. and Reese, Timothy G. and Wiegell, Mette R. and Makris, Nikos and Belliveau, John W. and Wedeen, Van J.},
    citeulike-article-id = {1241957},
    citeulike-linkout-0 = {http://dx.doi.org/10.1002/mrm.10268},
    citeulike-linkout-1 = {http://view.ncbi.nlm.nih.gov/pubmed/12353272},
    citeulike-linkout-2 = {http://www.hubmed.org/display.cgi?uids=12353272},
    citeulike-linkout-3 = {http://www3.interscience.wiley.com/cgi-bin/abstract/98518419/ABSTRACT},
    doi = {10.1002/mrm.10268},
    issn = {1522-2594},
    journal = {Magn. Reson. Med.},
    keywords = {similarity},
    month = oct,
    number = {4},
    pages = {577--582},
    pmid = {12353272},
    posted-at = {2012-03-19 17:05:13},
    priority = {2},
    publisher = {Wiley Subscription Services, Inc., A Wiley Company},
    title = {{High angular resolution diffusion imaging reveals intravoxel white matter fiber heterogeneity}},
    url = {http://dx.doi.org/10.1002/mrm.10268},
    volume = {48},
    year = {2002}
}

@inproceedings{dasgupta2000experiments,
    address = {San Francisco, CA, USA},
    author = {Dasgupta, Sanjoy},
    booktitle = {UAI '00: Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence},
    citeulike-article-id = {150307},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=719759},
    isbn = {1558607099},
    keywords = {similarity},
    pages = {143--151},
    posted-at = {2012-03-13 13:53:51},
    priority = {2},
    publisher = {Morgan Kaufmann Publishers Inc.},
    title = {{Experiments with Random Projection}},
    url = {http://portal.acm.org/citation.cfm?id=719759},
    year = {2000}
}

@inproceedings{hamerly2003learning,
    abstract = {{When clustering a dataset, the right number \$k\$ of clusters to use is often not obvious, and choosing \$k\$ automatically is a hard algorithmic problem. In this paper we present an improved algorithm for learning \$k\$ while clustering. The G-means algorithm is based on a statistical test for the hypothesis that a subset of data follows a Gaussian distribution. G-means runs \$k\$-means with increasing \$k\$ in a hierarchical fashion until the test accepts the hypothesis that the data assigned to each \$k\$-means center are Gaussian. Two key advantages are that the hypothesis test does not limit the covariance of the data and does not compute a full covariance matrix. Additionally, G-means only requires one intuitive parameter, the standard statistical significance level \$\alpha\$. We present results from experiments showing that the algorithm works well, and better than a recent method based on the BIC penalty for model complexity. In these experiments, we show that the BIC is ineffective as a scoring function, since it does not penalize strongly enough the model\&\#039;s complexity.}},
    author = {Hamerly, Greg and Elkan, Charles},
    booktitle = {In Neural Information Processing Systems},
    citeulike-article-id = {5346861},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.9.3574},
    keywords = {similarity},
    posted-at = {2012-03-02 16:49:14},
    priority = {2},
    title = {{Learning the K in K-Means}},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.9.3574},
    volume = {17},
    year = {2003}
}

@article{turnbull2005fast,
    abstract = {{This paper explores the automatic classification of audio tracks into musical genres. Our goal is to achieve human-level accuracy with fast training and classification. This goal is achieved with radial basis function (RBF) networks by using a combination of unsupervised and supervised initialization methods. These initialization methods yield classifiers that are as accurate as RBF networks trained with gradient descent (which is hundreds of times slower). In addition, feature subset selection further reduces training and classification time while preserving classification accuracy. Combined, our methods succeed in creating an RBF network that matches the musical classification accuracy of humans. The general algorithmic contribution of this paper is to show experimentally that RBF networks initialized with a combination of methods can yield good classification performance without relying on gradient descent. The simplicity and computational efficiency of our initialization methods produce classifiers that are fast to train as well as fast to apply to novel data. We also present an improved method for initializing the k-means clustering algorithm, which is useful for both unsupervised and supervised initialization methods.}},
    author = {Turnbull, D. and Elkan, C.},
    citeulike-article-id = {994347},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/TKDE.2005.62},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1401895},
    doi = {10.1109/TKDE.2005.62},
    institution = {Dept. of Comput. Sci. \& Eng., California Univ. San Diego, La Jolla, CA, USA},
    issn = {1041-4347},
    journal = {Knowledge and Data Engineering, IEEE Transactions on},
    keywords = {similarity},
    month = apr,
    number = {4},
    pages = {580--584},
    posted-at = {2012-02-10 15:32:45},
    priority = {2},
    publisher = {IEEE},
    title = {{Fast recognition of musical genres using RBF networks}},
    url = {http://dx.doi.org/10.1109/TKDE.2005.62},
    volume = {17},
    year = {2005}
}

@article{hochbaum1985best,
    abstract = {{Mathematics of Operations Research is a premier research journal emphasizing the mathematics in the broad field of operations research. It is a quarterly journal published by the Institute for Operations Research and the Management Sciences (INFORMS).}},
    author = {Hochbaum, Dorit S. and Shmoys, David B.},
    citeulike-article-id = {5860196},
    citeulike-linkout-0 = {http://dx.doi.org/10.1287/moor.10.2.180},
    citeulike-linkout-1 = {http://mor.journal.informs.org/content/10/2/180.abstract},
    citeulike-linkout-2 = {http://mor.journal.informs.org/content/10/2/180.full.pdf},
    day = {01},
    doi = {10.1287/moor.10.2.180},
    issn = {1526-5471},
    journal = {Mathematics of Operations Research},
    keywords = {similarity},
    month = may,
    number = {2},
    pages = {180--184},
    posted-at = {2012-02-10 15:28:51},
    priority = {2},
    publisher = {INFORMS},
    title = {{A Best Possible Heuristic for the k-Center Problem}},
    url = {http://dx.doi.org/10.1287/moor.10.2.180},
    volume = {10},
    year = {1985}
}

@article{forgy1965cluster,
    author = {Forgy, E. W.},
    citeulike-article-id = {3382461},
    journal = {Biometrics},
    keywords = {similarity},
    pages = {768--769},
    posted-at = {2012-02-10 14:41:58},
    priority = {2},
    title = {{Cluster analysis of multivariate data: efficiency vs interpretability of classifications}},
    volume = {21},
    year = {1965}
}

@article{pekalska2006prototype,
    abstract = {{A conventional way to discriminate between objects represented by dissimilarities is the nearest neighbor method. A more efficient and sometimes a more accurate solution is offered by other dissimilarity-based classifiers. They construct a decision rule based on the entire training set, but they need just a small set of prototypes, the so-called representation set, as a reference for classifying new objects. Such alternative approaches may be especially advantageous for non-Euclidean or even non-metric dissimilarities. The choice of a proper representation set for dissimilarity-based classifiers is not yet fully investigated. It appears that a random selection may work well. In this paper, a number of experiments has been conducted on various metric and non-metric dissimilarity representations and prototype selection methods. Several procedures, like traditional feature selection methods (here effectively searching for prototypes), mode seeking and linear programming are compared to the random selection. In general, we find out that systematic approaches lead to better results than the random selection, especially for a small number of prototypes. Although there is no single winner as it depends on data characteristics, the k-centres works well, in general. For two-class problems, an important observation is that our dissimilarity-based discrimination functions relying on significantly reduced prototype sets (3–10\% of the training objects) offer a similar or much better classification accuracy than the best k-NN rule on the entire training set. This may be reached for multi-class data as well, however such problems are more difficult.}},
    author = {Pekalska, E. and Duin, R. and Paclik, P.},
    booktitle = {Part Special Issue: Complexity Reduction},
    citeulike-article-id = {1698034},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/j.patcog.2005.06.012},
    citeulike-linkout-1 = {http://www.sciencedirect.com/science/article/B6V14-4H27BWG-1/2/ef717bd34157251ea9aee3bdeacfa052},
    doi = {10.1016/j.patcog.2005.06.012},
    issn = {00313203},
    journal = {Pattern Recognition},
    keywords = {similarity},
    month = feb,
    number = {2},
    pages = {189--208},
    posted-at = {2012-02-10 14:27:25},
    priority = {2},
    title = {{Prototype selection for dissimilarity-based classifiers}},
    url = {http://dx.doi.org/10.1016/j.patcog.2005.06.012},
    volume = {39},
    year = {2006}
}

@article{linial1995geometry,
    abstract = {{In this paper we explore some implications of viewing graphs asgeometric objects. This approach offers a new perspective on a number of graph-theoretic and algorithmic problems. There are several ways to model graphs geometrically and our main concern here is with geometric representations that respect themetric of the (possibly weighted) graph. Given a graphG we map its vertices to a normed space in an attempt to (i) keep down the dimension of the host space, and (ii) guarantee a smalldistortion, i.e., make sure that distances between vertices inG closely match the distances between their geometric images.}},
    author = {Linial, Nathan and London, Eran and Rabinovich, Yuri},
    citeulike-article-id = {6995166},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/BF01200757},
    citeulike-linkout-1 = {http://www.springerlink.com/content/h16248312kq1t054},
    day = {21},
    doi = {10.1007/BF01200757},
    issn = {0209-9683},
    journal = {Combinatorica},
    keywords = {similarity},
    month = jun,
    number = {2},
    pages = {215--245},
    posted-at = {2012-02-08 16:59:04},
    priority = {2},
    publisher = {Springer Berlin / Heidelberg},
    title = {{The geometry of graphs and some of its algorithmic applications}},
    url = {http://dx.doi.org/10.1007/BF01200757},
    volume = {15},
    year = {1995}
}

@inproceedings{abraham2007local,
    abstract = {{In many application areas, complex data sets are often representedby some metric space and metric embedding is used to provide a more structured representation of the data. In many of these applications much greater emphasis is put on the preserving the local structure of the original space than on maintaining its complete structure. This is also the case in some networking applications where "small world" phenomena in communication patterns has been observed. Practical study of embedding has indeed involved with finding embeddings with this property. In this paper we initiate thestudy of local embeddings of metric spaces and provide embeddings with distortion depending solely on the local structureof the space.}},
    address = {New York, NY, USA},
    author = {Abraham, Ittai and Bartal, Yair and Neiman, Ofer},
    booktitle = {Proceedings of the thirty-ninth annual ACM symposium on Theory of computing},
    citeulike-article-id = {10327581},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1250883},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1250790.1250883},
    doi = {10.1145/1250790.1250883},
    isbn = {978-1-59593-631-8},
    keywords = {similarity},
    location = {San Diego, California, USA},
    pages = {631--640},
    posted-at = {2012-02-08 16:53:19},
    priority = {2},
    publisher = {ACM},
    series = {STOC '07},
    title = {{Local embeddings of metric spaces}},
    url = {http://dx.doi.org/10.1145/1250790.1250883},
    year = {2007}
}

@article{pekalska2002generalized,
    abstract = {{Usually, objects to be classified are represented by features. In this paper, we discuss an alternative object representation based on dissimilarity values. If such distances separate the classes well, the nearest neighbor method offers a good solution. However, dissimilarities used in practice are usually far from ideal and the performance of the nearest neighbor rule suffers from its sensitivity to noisy examples. We show that other, more global classification techniques are preferable to the nearest neighbor rule, in such cases.For classification purposes, two different ways of using generalized dissimilarity kernels are considered. In the first one, distances are isometrically embedded in a pseudo-Euclidean space and the classification task is performed there. In the second approach, classifiers are built directly on distance kernels. Both approaches are described theoretically and then compared using experiments with different dissimilarity measures and datasets including degraded data simulating the problem of missing values.}},
    address = {Cambridge, MA, USA},
    author = {Pekalska, Elzbieta and Paclik, Pavel and Duin, Robert P. W.},
    citeulike-article-id = {3823737},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=944810},
    issn = {1532-4435},
    journal = {J. Mach. Learn. Res.},
    keywords = {similarity},
    pages = {175--211},
    posted-at = {2012-02-08 16:13:59},
    priority = {2},
    publisher = {JMLR.org},
    title = {{A generalized kernel approach to dissimilarity-based classification}},
    url = {http://portal.acm.org/citation.cfm?id=944810},
    volume = {2},
    year = {2002}
}

@inproceedings{hwang2011convergent,
    abstract = {{Given a set V of n vectors in d-dimensional space, we provide an efficient method for computing quality upper and lower bounds of the Euclidean distances between a pair of the vectors in V . For this purpose, we define a distance measure, called the MS-distance, by using the mean and the standard deviation values of vectors in V . Once we compute the mean and the standard deviation values of vectors in V in O(dn) time, the MS-distance between them provides upper and lower bounds of Euclidean distance between a pair of vectors in V in constant time. Furthermore, these bounds can be refined further such that they converge monotonically to the exact Euclidean distance within d refinement steps. We also provide an analysis on a random sequence of refinement steps which can justify why MS-distance should be refined to provide very tight bounds in a few steps of a typical sequence. The MS-distance can be used to various problems where the Euclidean distance is used to measure the proximity or similarity between objects. We provide experimental results on the nearest and the farthest neighbor searches.}},
    author = {Hwang, Yoonho and Ahn, Hee-Kap},
    booktitle = {Neural Information Processing Systems 25},
    citeulike-article-id = {10176574},
    citeulike-linkout-0 = {http://books.nips.cc/papers/files/nips24/NIPS2011\_0286.pdf},
    keywords = {similarity},
    posted-at = {2011-12-28 15:55:00},
    priority = {2},
    title = {{Convergent Bounds on the Euclidean Distance}},
    url = {http://books.nips.cc/papers/files/nips24/NIPS2011\_0286.pdf},
    year = {2011}
}

@inproceedings{beygelzimer2006cover,
    abstract = {{We present a tree data structure for fast nearest neighbor operations in general n-point metric spaces (where the data set consists of n points). The data structure requires O(n) space regardless of the metric's structure yet maintains all performance properties of a navigating net (Krauthgamer \& Lee, 2004b). If the point set has a bounded expansion constant c, which is a measure of the intrinsic dimensionality, as defined in (Karger \& Ruhl, 2002), the cover tree data structure can be constructed in O (c6n log n) time. Furthermore, nearest neighbor queries require time only logarithmic in n, in particular O (c12 log n) time. Our experimental results show speedups over the brute force search varying between one and several orders of magnitude on natural machine learning datasets.}},
    address = {New York, NY, USA},
    author = {Beygelzimer, Alina and Kakade, Sham and Langford, John},
    booktitle = {Proceedings of the 23rd international conference on Machine learning},
    citeulike-article-id = {1283928},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1143857},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1143844.1143857},
    doi = {10.1145/1143844.1143857},
    isbn = {1-59593-383-2},
    keywords = {covertree, machinelearning, similarity},
    location = {Pittsburgh, Pennsylvania},
    pages = {97--104},
    posted-at = {2011-12-28 14:38:38},
    priority = {2},
    publisher = {ACM},
    series = {ICML '06},
    title = {{Cover trees for nearest neighbor}},
    url = {http://dx.doi.org/10.1145/1143844.1143857},
    year = {2006}
}

@article{witten2010framework,
    abstract = {{
                We consider the problem of clustering observations using a potentially large set of features. One might expect that the true underlying clusters present in the data differ only with respect to a small fraction of the features, and will be missed if one clusters the observations using the full set of features. We propose a novel framework for sparse clustering, in which one clusters the observations using an adaptively chosen subset of the features. The method uses a lasso-type penalty to select the features. We use this framework to develop simple methods for sparse K-means and sparse hierarchical clustering. A single criterion governs both the selection of the features and the resulting clusters. These approaches are demonstrated on simulated data and on genomic data sets.
            }},
    author = {Witten, Daniela M. and Tibshirani, Robert},
    citeulike-article-id = {7789095},
    citeulike-linkout-0 = {http://dx.doi.org/10.1198/jasa.2010.tm09415},
    citeulike-linkout-1 = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2930825/},
    citeulike-linkout-2 = {http://view.ncbi.nlm.nih.gov/pubmed/20811510},
    citeulike-linkout-3 = {http://www.hubmed.org/display.cgi?uids=20811510},
    day = {1},
    doi = {10.1198/jasa.2010.tm09415},
    issn = {0162-1459},
    journal = {Journal of the American Statistical Association},
    keywords = {machinelearning, similarity},
    month = jun,
    number = {490},
    pages = {713--726},
    pmcid = {PMC2930825},
    pmid = {20811510},
    posted-at = {2011-12-23 12:57:36},
    priority = {2},
    title = {{A framework for feature selection in clustering.}},
    url = {http://dx.doi.org/10.1198/jasa.2010.tm09415},
    volume = {105},
    year = {2010}
}

@inproceedings{sculley2010kmeans,
    abstract = {{We present two modifications to the popular k-means clustering algorithm to address the extreme requirements for latency, scalability, and sparsity encountered in user-facing web applications. First, we propose the use of mini-batch optimization for k-means clustering. This reduces computation cost by orders of magnitude compared to the classic batch algorithm while yielding significantly better solutions than online stochastic gradient descent. Second, we achieve sparsity with projected gradient descent, and give a fast ε-accurate projection onto the L1-ball. Source code is freely available: http://code.google.com/p/sofia-ml}},
    address = {New York, NY, USA},
    author = {Sculley, D.},
    booktitle = {Proceedings of the 19th international conference on World wide web},
    citeulike-article-id = {10163352},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1772862},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1772690.1772862},
    doi = {10.1145/1772690.1772862},
    isbn = {978-1-60558-799-8},
    keywords = {machinelearning, similarity},
    location = {Raleigh, North Carolina, USA},
    pages = {1177--1178},
    posted-at = {2011-12-23 12:22:42},
    priority = {2},
    publisher = {ACM},
    series = {WWW '10},
    title = {{Web-scale k-means clustering}},
    url = {http://dx.doi.org/10.1145/1772690.1772862},
    year = {2010}
}

@electronic{omohundro1989balltree,
    abstract = {{Abstract. Balltrees are simple geometric data structures with a wide range of practical applications to geometric learning tasks. In this report we compare 5 different algorithms for constructing balltrees from data. We study the trade-off between construction time and the quality of the constructed tree. Two of the algorithms are on-line, two construct the structures from the data set in a top down fashion, and one uses a bottom up approach. We empirically study the algorithms on random data drawn from eight different probability distributions representing smooth, clustered, and curve distributed data in different ambient space dimensions. We find that the bottom up approach usually produces the best trees but has the longest construction time. The other approaches have uses in specific circumstances. 1}},
    author = {Omohundro, Stephen M.},
    citeulike-article-id = {6043596},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.91.8209},
    keywords = {machinelearning, similarity, tree},
    posted-at = {2011-12-23 11:33:06},
    priority = {2},
    title = {{Five Balltree Construction Algorithms}},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.91.8209},
    year = {1989}
}

@article{balcan2008theory,
    abstract = {{Abstract\&nbsp;\&nbsp;Kernel functions have become an extremely popular tool in machine learning, with an attractive theory as well. This theory views a kernel as implicitly mapping data points into a possibly very high dimensional space, and describes a kernel function as being good for a given learning problem if data is separable by a large margin in that implicit space. However, while quite elegant, this theory does not necessarily correspond to the intuition of a good kernel as a good measure of similarity, and the underlying margin in the implicit space usually is not apparent in  ” natural” representations of the data. Therefore, it may be difficult for a domain expert to use the theory to help design an appropriate kernel for the learning task at hand. Moreover, the requirement of positive semi-definiteness may rule out the most natural pairwise similarity functions for the given problem domain. In this work we develop an alternative, more general theory of learning with similarity functions (i.e., sufficient conditions for a similarity function to allow one to learn well) that does not require reference to implicit spaces, and does not require the function to be positive semi-definite (or even symmetric). Instead, our theory talks in terms of more direct properties of how the function behaves as a similarity measure. Our results also generalize the standard theory in the sense that any good kernel function under the usual definition can be shown to also be a good similarity function under our definition (though with some loss in the parameters). In this way, we provide the first steps towards a theory of kernels and more general similarity functions that describes the effectiveness of a given function in terms of natural similarity-based properties.}},
    author = {Balcan, Maria-Florina and Blum, Avrim and Srebro, Nathan},
    citeulike-article-id = {2810130},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/s10994-008-5059-5},
    citeulike-linkout-1 = {http://www.springerlink.com/content/7228822688554727},
    day = {1},
    doi = {10.1007/s10994-008-5059-5},
    journal = {Machine Learning},
    keywords = {machinelearning, similarity},
    month = aug,
    number = {1},
    pages = {89--112},
    posted-at = {2011-12-23 11:03:40},
    priority = {2},
    title = {{A theory of learning with similarity functions}},
    url = {http://dx.doi.org/10.1007/s10994-008-5059-5},
    volume = {72},
    year = {2008}
}

@article{chen2009similarity,
    abstract = {{This paper reviews and extends the field of similarity-based classification, presenting new analyses, algorithms, data sets, and a comprehensive set of experimental results for a rich collection of classification problems. Specifically, the generalizability of using similarities as features is analyzed, design goals and methods for weighting nearest-neighbors for similarity-based learning are proposed, and different methods for consistently converting similarities into kernels are compared. Experiments on eight real data sets compare eight approaches and their variants to similarity-based learning.}},
    author = {Chen, Yihua and Garcia, Eric K. and Gupta, Maya R. and Rahimi, Ali and Cazzanti, Luca},
    citeulike-article-id = {4278446},
    citeulike-linkout-0 = {http://www.jmlr.org/papers/volume10/chen09a/chen09a.pdf},
    journal = {Journal of Machine Learning Research},
    keywords = {machinelearning, similarity},
    month = mar,
    pages = {747--776},
    posted-at = {2011-12-23 11:01:31},
    priority = {2},
    title = {{Similarity-based Classification: Concepts and Algorithms}},
    url = {http://www.jmlr.org/papers/volume10/chen09a/chen09a.pdf},
    volume = {10},
    year = {2009}
}

@article{mori2002fiber,
    abstract = {{The state of the art of reconstruction of the axonal tracts in the central nervous system (CNS) using diffusion tensor imaging (DTI) is reviewed. This relatively new technique has generated much enthusiasm and high expectations because it presently is the only approach available to non-invasively study the three-dimensional architecture of white matter tracts. While there is no doubt that DTI fiber tracking is providing exciting new opportunities to study CNS anatomy, it is very important to understand its limitations. In this review we therefore assess the basic principles and the assumptions that need to be made for each step of the study, including both data acquisition and the elaborate fiber reconstruction algorithms. Special attention is paid to situations where complications may arise, and possible solutions are reviewed. Validation issues and potential future directions and improvements are also discussed. Copyright {\copyright} 2002 John Wiley \& Sons, Ltd.}},
    address = {Johns Hopkins University School of Medicine, Department of Radiology and Radiological Science and Kennedy Krieger Institute, F.M. Kirby Research Center for Functional Brain Imaging, Baltimore, MD 21205, USA},
    author = {Mori, Susumu and van Zijl, Peter C. M.},
    citeulike-article-id = {591563},
    citeulike-linkout-0 = {http://dx.doi.org/10.1002/nbm.781},
    citeulike-linkout-1 = {http://view.ncbi.nlm.nih.gov/pubmed/12489096},
    citeulike-linkout-2 = {http://www.hubmed.org/display.cgi?uids=12489096},
    citeulike-linkout-3 = {http://www3.interscience.wiley.com/cgi-bin/abstract/101521422/ABSTRACT},
    doi = {10.1002/nbm.781},
    issn = {1099-1492},
    journal = {NMR Biomed.},
    keywords = {dmrireport, similarity},
    number = {7-8},
    pages = {468--480},
    pmid = {12489096},
    posted-at = {2011-12-14 16:02:25},
    priority = {2},
    publisher = {John Wiley \& Sons, Ltd.},
    title = {{Fiber tracking: principles and strategies – a technical review}},
    url = {http://dx.doi.org/10.1002/nbm.781},
    volume = {15},
    year = {2002}
}

@article{basser1994diffusion,
    abstract = {{
                This paper describes a new NMR imaging modality--MR diffusion tensor imaging. It consists of estimating an effective diffusion tensor, Deff, within a voxel, and then displaying useful quantities derived from it. We show how the phenomenon of anisotropic diffusion of water (or metabolites) in anisotropic tissues, measured noninvasively by these NMR methods, is exploited to determine fiber tract orientation and mean particle displacements. Once Deff is estimated from a series of NMR pulsed-gradient, spin-echo experiments, a tissue's three orthotropic axes can be determined. They coincide with the eigenvectors of Deff, while the effective diffusivities along these orthotropic directions are the eigenvalues of Deff. Diffusion ellipsoids, constructed in each voxel from Deff, depict both these orthotropic axes and the mean diffusion distances in these directions. Moreover, the three scalar invariants of Deff, which are independent of the tissue's orientation in the laboratory frame of reference, reveal useful information about molecular mobility reflective of local microstructure and anatomy. Inherently tensors (like Deff) describing transport processes in anisotropic media contain new information within a macroscopic voxel that scalars (such as the apparent diffusivity, proton density, T1, and T2) do not.
            }},
    author = {Basser, P. J. and Mattiello, J. and LeBihan, D.},
    citeulike-article-id = {1685696},
    citeulike-linkout-0 = {http://dx.doi.org/10.1016/S0006-3495(94)80775-1},
    citeulike-linkout-1 = {http://www.biophysj.org/cgi/content/abstract/66/1/259},
    citeulike-linkout-2 = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1275686/},
    citeulike-linkout-3 = {http://view.ncbi.nlm.nih.gov/pubmed/8130344},
    citeulike-linkout-4 = {http://www.hubmed.org/display.cgi?uids=8130344},
    day = {1},
    doi = {10.1016/S0006-3495(94)80775-1},
    issn = {0006-3495},
    journal = {Biophysical journal},
    keywords = {dmrireport, similarity},
    month = jan,
    number = {1},
    pages = {259--267},
    pmcid = {PMC1275686},
    pmid = {8130344},
    posted-at = {2011-11-25 13:38:14},
    priority = {2},
    title = {{MR diffusion tensor spectroscopy and imaging.}},
    url = {http://dx.doi.org/10.1016/S0006-3495(94)80775-1},
    volume = {66},
    year = {1994}
}

