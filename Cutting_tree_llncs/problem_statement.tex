\section{Problem statement}
\label{sec:problem}
In this part, after introducing the hierarchical clustering we formally describe the problem of 
multiple scale representation.

\vspace{-2.75mm}
\subsection{Hierarchical clustering}
\label{subsec:hierarchical}
Given a set of input patterns denoted as $\mathcal{X} = \{\mathsf{x_1}, \ldots, \mathsf{x_j}, \ldots, \mathsf{x_N}\}$ where each data point $\mathsf{x_j} = (x_{j1},x_{j2}, \ldots,x_{jd})^T \in \mathfrak{R}^d$ and each measure $x_{ji}$ is said to be a feature (attribute, dimension, or variable). A hierarchical tree (or dendrogram) of $\mathcal{X}$ is defined as following:
\begin{definition}
\label{def:hierarchical_tree}
A \textit{\textbf{hierarchical tree}} $\mathcal{H}$ of an N-object set $\mathcal{X}=\{\mathsf{x_1}, \ldots, \mathsf{x_j}, \ldots, \mathsf{x_N}\}$ is a collection of $Q$ partitions on $\mathcal{X}$: $\mathcal{H}=\{\mathsf{P}_0,\ldots,\mathsf{P}_Q\}$, with $Q \leq N$, such that $\mathsf{P}_0=\mathcal{X}$ and $C_{i} \in P_{m}, C_{j} \in P_{l}, m > l$ imply $C_{i} \subseteq C_{j}$ or $C_{i} \cap C_{j} = \emptyset$, for all $i,j \neq i, m, l = 1, \ldots, Q$.
\end{definition}
The hierarchical clustering algorithm~\cite{johnson1967hierarchical} builds nested clusters by merging them successively, and this hierarchy of clusters represented as a tree/dendrogram. The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. It produces a structure of clusters of $\mathcal{X}$ that is more informative than the unstructured set of clusters returned by flat clustering. This characteristic meets the requirement of creating multiple scales of one original dataset $\mathcal{X}=\{\mathsf{x_1}, \ldots, \mathsf{x_j}, \ldots, \mathsf{x_N}\}$ 
%$\mathbb{P}$ of $\mathcal{T}$ 
%the immediately responding to the changing level of abstraction of user 
without re-running the clustering algorithm again.
%Another advantage is that hierarchical clustering does not require to pre-specify the number of clusters.
%In another way, hierarchical clustering algorithm % (http://scikit-learn.org/stable/modules/clustering.html)  can clusters data firstly on $n$ centers and consequently until only one center. 
Moreover, it leads to the capability of visualizing $\mathcal{X}$ in many levels of abstraction, and the users can browse the value of level from $1$ to $N$, to see the clusters immediately.
%We refer \mathcal{H} as the whole hierarchical tree or dendogram of $\mathcal{X}$.

%Every hierarchical tree $\mathcal{H}$ has a branch factor parameter $\gamma$ that quantifies how balanced the clusters are at any split. Formally, $\gamma \geq \max_{intras C_i \in \{C_1, \ldots,C_k\}} \frac{max_i |C_i|}{min_i |C_i|}$ where each intra $C-i$ is a non-leaf cluster
 %, partitioned into clusters ${C_1, \ldots, C_k}$. 
 %$\gamma$ upper bounds the ratio between the largest and smallest clusters sizes across all intras in cluster $C$. This branch factor has been used in numerous analyses of clustering algorithms~\cite{eriksson2011active,balakrishnan2011noise}, and it is common to assume that the clustering is not too unbalanced.

Hierarchical clustering algorithms are either top down or bottom up. Bottom-up algorithms treat each streamline as a singleton cluster at the outset and then successively merge (or agglomerate) pairs of clusters until all clusters have been merged into a single cluster that contains all tracts. Bottom-up hierarchical clustering is therefore called Hierarchical Agglomerative Clustering (HAC). Top-down clustering requires a method for splitting a cluster. It proceeds by splitting clusters recursively until individual streamlines are reached \cite{johnson1967hierarchical}.

%The basic process of HAC clustering is this:
%\begin{enumerate}
%	\item Assign each object to one cluster, and get the result of $N$ clusters, each of them contains just one streamline.
	%Let the distances (similarities) between the clusters the same as the distances (similarities) between the items they contain.
%	\item Find the closest (most similar) pair of clusters and merge them into a single cluster.
%	\item Compute distances (similarities) between the new cluster and each of the old clusters.
%	\item Repeat steps $2$ and $3$ until all objects are clustered into a single cluster of size $n$.
%\end{enumerate}

%Step $3$ can be done in different ways, which distinguishes single-linkage, complete-linkage and average-linkage (detail can be found in~\cite{johnson1967hierarchical}).
%In \emph{single-linkage} clustering (also called the connectedness or minimum method), the distance between a pair of clusters $A$ and $B$ is the shortest distance from any streamline of one cluster to any streamline of the other cluster. 
%%If the data consist of similarities, we consider the similarity between one cluster and another cluster to be equal to the greatest similarity from any member of one cluster to any member of the other cluster.
%${d}_sg(A,B) = \min_{i=1,\ldots,n_{s_A}} d({x}_i^A, s_B)$
%\begin{equation}
%\label{eq:distance_single_linkage}
%d(A, B) = \min_{\mathsf{x}_A \in {A},\mathsf{x}_B \in {B}} d(\mathsf{x}_A,\mathsf{x}_B)
%\end{equation}
%where $d(\mathsf{x}_A,\mathsf{x}_B)$ is the distance between two objects
%In \emph{complete-linkage} clustering (also called the diameter or maximum method), we consider the distance between cluster $A$ and cluster $B$ to be equal to the greatest distance from any member of one cluster to any member of the other cluster.
%\begin{equation}
%\label{eq:distance_complete_linkage}
%d(A, B) = \max_{\mathsf{x}_A \in {A},\mathsf{x}_B \in {B}} d(\mathsf{x}_A,\mathsf{x}_B)
%\end{equation}
%In \emph{average-linkage} clustering, the distance between two clusters $A$ and $B$ is defined as the average distance from any streamline of cluster $A$ to any streamline of cluster $B$.
%\begin{equation}
%\label{eq:distance_average_linkage}
%d(A, B) = avg_{\mathsf{x}_A \in {A},\mathsf{x}_B \in {B}} d(\mathsf{x}_A,\mathsf{x}_B)
%\end{equation}
%\algsetup{indent=2em}
%%\newcommand{\factorial}{\ensuremath{\mbox{\sc Factorial}}}
%\begin{algorithm}[h!]
%\caption{Hierarchical clustering}\label{alg:hierarchical}
%\begin{algorithmic}[1]
%	\REQUIRE $\mathcal{X}=\{\mathsf{x_1}, \ldots, \mathsf{x_j}, \ldots, \mathsf{x_N}\}$
%	\ENSURE \textit{\textbf{hierarchical tree}} $\mathcal{H}$
%	%\REQUIRE An integer $n \geq 0$.
%	%\ENSURE The value of $n!$.
%	\medskip
%	\STATE $C_i \leftarrow \mathsf{x_i}, \forall i \in [1,\ldots, N]$
%	\vspace{1.5mm}
%	\STATE $\mathcal{H} \leftarrow \bigcup_{i=1}^{N}C_i$
%	\vspace{1.3mm}
%	\WHILE {$N > 1$}
%		\vspace{1.2mm}
%		\STATE $(C_a, C_b) \leftarrow agrmin(d(C_i,C_j)),$\\
%		\hspace{19mm} $ \forall C_i \in \mathcal{H},C_j \in \mathcal{H}, C_i \neq C_j$
%		\vspace{1.2mm}
%		\STATE $\mathcal{H} \leftarrow (C_a \cup C_b)$
%		\vspace{1.2mm}
%		\STATE $N \leftarrow N - 1$	
%	\ENDWHILE
%	\vspace{1.3mm}
%	\RETURN $\mathcal{H}$
%\end{algorithmic}
%\end{algorithm}
%After defining the distance measure between two clusters, the hierarchical procedure can easily done as the pseudo code in the algorithm~\ref{alg:hierarchical}.

\vspace{-2.75mm}
\subsection{Multiple scales for visualization}
\label{subsec:multiple_scales}
\vspace{-2mm}
The hierarchical tree $\mathcal{H}$ structures and presents dataset $\mathcal{X}$ at different levels of abstraction. A non-leaf cluster is composed of all its child clusters, while a leaf cluster contains only a single data item. The collection of all leaf-clusters presents exactly every data items $\mathsf{x}_i$ of $\mathcal{X}$, while the root is a cluster containing whole dataset $\mathcal{X}$ as one single node of the tree.
\begin{definition}
\label{def:level_detail}
Each cluster $C_i$ (node) of the tree $\mathcal{H}$, let $s(C_i)$ be the \textbf{\textit{level of detail}} of that cluster. This measurement $s(C_i)$ satisfies the following criteria: if $C_i$ is an ancestor of $C_j$, then $s(C_i) \geq s(C_j)$. 
\end{definition}
There are many properties of a cluster which could be used to measure $s(C_i)$. Among these, two common uses are the radius of a cluster (maximum distance between all pair samples of cluster $C_i$: $r_i = max_{\forall \mathsf{x}_a, \mathsf{x}_b
 \in C_i, \mathsf{x}_a \neq \mathsf{x}_b} \{d(\mathsf{x}_a,\mathsf{x}_b)\}$); and the hierarchical level of $C_i$ in the tree $\mathcal{H}$~\cite{yang2003interactive}: $s(C_i) = \frac{height(C_i)}{h}$, where $heigh(C_i)$ is the heigh of the cluster $C_i$, and $h$ is the heigh of the tree $\mathcal{H}$.

\begin{definition}
\label{def:range_scale}
 The \textbf{\textit{range of scale}} of a hierarchical tree $\mathcal{H}$ is $[s_{min},s_{max}]$, where $s_{min} = \min_{\forall C_i \in \mathcal{H}}\{s(C_i)\}$, and $s_{max} = \max_{\forall C_i \in \mathcal{H}}\{s(C_i)\}$
\end{definition}
Depending on which property is used to measure the level of detail $s_i$, the value of  $s_{max}$ and  $s_{min}$ would be different. In the case of using the hierarchical level, the scale range is from $[0,1]$, where $s_{min} = 0$ corresponds to the leaf with zero heigh, to $s_{max} = 1$ is at the root of the tree $\mathcal{H}$. However, in the case of using cluster radius, there is no guarantee that $s_{min} = 0$ and $s_{max} = 1$.

\begin{definition}
\label{def:cut}
 \textbf{\textit{A cut $\mathfrak{L}$}} of a hierarchical tree $\mathcal{H}$ at a given scale $w \in [s_{min},s_{max}] $  is $\mathfrak{L}(w)$:
\begin{equation}
\label{equ:a_cut_l}
\mathfrak{L}(w) = \{C_i | (s(C_i) \leq w  \wedge s(parent(C_i)) > w)\}
\end{equation}
where $parent(C_i)$ is the direct parent node of the cluster $C_i$
\end{definition}
In general, $\mathfrak{L}(w)$ is a partition of $\mathcal{X}$, denoting a subset of the tree $\mathcal{H}$. The cut at $s_{min}$, $\mathfrak{L}(s_{min})$ is a set of all leaf clusters, while the $\mathfrak{L}(s_{max})$ is a single cluster representing the whole dataset $\mathcal{X}$. Intuitively, $\mathfrak{L}(w)$ changes smoothly with the variance of the scale parameter $w$, which serves as the abstraction level of the dataset $\mathcal{X}$. It could be imagined that $\mathfrak{L}(w)$ is a cut across a vertically oriented hierarchical tree $\mathcal{H}$ that satisfies criteria: $\mathfrak{L}(w)$ intersects each path of the tree $\mathcal{H}$, from the root to the leaf, only exactly at one point. The cutting point would depend on the value of parameter $w$. It should close to the root of the tree $\mathcal{H}$ when $w$ is high, and reversely. Moreover, the cut can be horizontal or unhorizontal (like zigzag) as long as for each path from the root to the leaf of the tree $\mathcal{H}$, there is only one crossing with $\mathfrak{L}(w)$. It is an open approach for cutting the tree comparing with the traditional one which only accepts the horizontal cut. 

\begin{definition}
\label{def:nested_in} 
Let $P$ and $Q$ be two partitions of dataset $\mathcal{X}$, $P = \{C_{1}^{P}, \ldots, C_{l}^{P}\}$ and $Q = \{C_{1}^{Q}, \ldots, C_{m}^{Q}\}$. Partition $P$ is \textbf{\textit{nested in}} partition $Q$, denoted as $P \preceq Q$, if and only if
\begin{equation}
\label{equ:nested_in}
P \preceq Q  \leftrightarrow \forall C_i^{Q} \in Q, \exists C^{P}_{i_{1}},\ldots, C^{P}_{i_{k}} \in P: C_i^Q = \cup_{t=1}^{k} C^{P}_{i_{t}}
\end{equation}
\end{definition}
\begin{definition}
\label{def:multi_scales_representation} Given the scale range $[s_{min},s_{max}]$ of a tree $\mathcal{H}$, the \textbf{\textit{multiple scales representation}} for the tree $\mathcal{H}$ is an \emph{ordered set} of $k$ scale values from $[s_{min},s_{max}]$: $\mathsf{\textit{B}} = \{b_1, b_2, \ldots, b_k\}, b_i \in [s_{min},s_{max}], \forall i \in [1,k]$, where $k$ is the order of set $\mathsf{\textit{B}}$, which satisfies the following condition:
\begin{equation}
\label{equ:multi_scales_represent}
\forall i \in [1, \ldots, k-1] : \mathfrak{L}(b_i)  \preceq \mathfrak{L}(b_{i+1})
\end{equation}
\end{definition}
\vspace{1mm}
\textit{\textbf{Multiple scale representation problem}}: Given a hierarchical tree $\mathcal{H}$ on a dataset $\mathcal{X}$, with the scale range $[s_{min},s_{max}]$. How to choose the multiple scales representing for the tree $\mathcal{H}$: $\mathsf{\textit{B}} = \{b_1, b_2, \ldots, b_k\}, b_i \in [s_{min},s_{max}], \forall i \in [1,k]$?

\vspace{2mm}
\noindent It is an $NP-problem$, and there is no general solution for it. Usually, it is chosen that $b_1 = s_{min}$, where the whole elements of $\mathcal{X}$ are presented, and $b_k = s_{max}$, which corresponds to only one virtual representation of $\mathcal{X}$. However, the value of $k$ is an open question and totally depends on the application. In the next section, we will discuss about how to define the $k$ value and also how to select each $b_i$ from $[s_{min},s_{max}]$.
