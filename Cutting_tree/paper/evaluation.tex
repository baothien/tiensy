\section{Evaluation}
\label{sec:evaluation}
Before proceeding with our main experiments, we first clarify some notation and introduce the hierarchical model the we will analyze. 
\subsection{Hierarchical clustering}
\label{subsec:hierarchical}
Given a set of input patterns denoted as $\mathcal{X} = \{\mathsf{x_1}, \ldots, \mathsf{x_j}, \ldots, \mathsf{x_N}\}$ where $\mathsf{x_j} = (x_{j1},x_{j2}, \ldots,x_{jd})^T \in \mathfrak{R}^d$ and each measure $x_{ji}$ is said to be a feature (attribute, dimension, or variable). Hierarchical clustering~\cite{johnson1967hierarchical} produces a structure of clusters of $\mathcal{X}$ that is more informative than the unstructured set of clusters returned by flat clustering. 
This characteristic meets the requirement of creating multiple scales of one original dataset $\mathcal{X}=\{x_1, \ldots, x_n\}$ 
%$\mathbb{P}$ of $\mathcal{T}$ 
%the immediately responding to the changing level of abstraction of user 
without re-running the clustering algorithm again. Another advantage is that hierarchical clustering does not require to pre-specify the number of clusters. It builds nested clusters by merging them successively, and this hierarchy of clusters represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. In another way, hierarchical clustering algorithm % (http://scikit-learn.org/stable/modules/clustering.html) 
can clusters data firstly on $n$ centers and consequently until only one center. This main character leads to the capability of visualizing $\mathcal{X}$ in many levels of abstraction, and the users can browse the value of level from $1$ to $N$, to see the clusters immediately.
%We refer \mathcal{H} as the whole hierarchical tree or dendogram of $\mathcal{X}$.

\textbf{Definition 1} A \textit{\textbf{hierarchical tree}} $\mathcal{H}$ on objects $\mathcal{X}=\{\mathsf{x_1}, \ldots, \mathsf{x_j}, \ldots, \mathsf{x_N}\}$ is a collection of $Q$ partitions on $\mathcal{X}$: $\mathcal{H}=\{\mathsf{P}_0,\ldots,\mathsf{P}_Q\}$, with $Q \leq N$, such that $\mathsf{P}_0=\mathcal{X}$ and $C_{i} \in P_{m}, C_{j} \in P_{l}, m > l$ imply $C_{i} \subseteq C_{j}$ or $C_{i} \cap C_{j} = \emptyset$, for all $i,j \neq i, m, l = 1, \ldots, Q$.

Every hierarchical tree $\mathcal{H}$ has a branch factor parameter $\gamma$ that quantifies how balanced the clusters are at any split.
Formally, $\gamma \geq \max_{intras C_i \in \{C_1, \ldots,C_k\}} \frac{max_i |C_i|}{min_i |C_i|}$
 where each intra $C-i$ is a non-leaf cluster
 %, partitioned into clusters ${C_1, \ldots, C_k}$. 
 $\gamma$ upper bounds the ratio between the largest and smallest clusters sizes across all intras in cluster $C$. This branch factor has been used in numerous analyses of clustering algorithms~\cite{eriksson2011active,balakrishnan2011noise}, and it is common to assume that the clustering is not too unbalanced.

Hierarchical clustering algorithms are either top-down or bottom-up. Bottom-up algorithms treat each streamline as a singleton cluster at the outset and then successively merge (or agglomerate) pairs of clusters until all clusters have been merged into a single cluster that contains all tracts. Bottom-up hierarchical clustering is therefore called Hierarchical Agglomerative Clustering (HAC). Top-down clustering requires a method for splitting a cluster. It proceeds by splitting clusters recursively until individual streamlines are reached \cite{johnson1967hierarchical}.
The basic process of HAC clustering~\cite{johnson1967hierarchical} is this:
\begin{enumerate}
	\item Assign each object to one cluster, and get the result of $N$ clusters, each of them contains just one streamline.
	%Let the distances (similarities) between the clusters the same as the distances (similarities) between the items they contain.
	\item Find the closest (most similar) pair of clusters and merge them into a single cluster.
	\item Compute distances (similarities) between the new cluster and each of the old clusters.
	\item Repeat steps $2$ and $3$ until all streamlines are clustered into a single cluster of size $n$.
\end{enumerate}
Step $3$ can be done in different ways, which distinguishes single-linkage, complete-linkage and average-linkage.
In \emph{single-linkage} clustering (also called the connectedness or minimum method), the distance between a pair of clusters $A$ and $B$ is the shortest distance from any streamline of one cluster to any streamline of the other cluster. 
%If the data consist of similarities, we consider the similarity between one cluster and another cluster to be equal to the greatest similarity from any member of one cluster to any member of the other cluster.
%${d}_sg(A,B) = \min_{i=1,\ldots,n_{s_A}} d({x}_i^A, s_B)$
\begin{equation}
\label{eq:distance_single_linkage}
d(A, B) = \min_{\mathsf{x}_A \in {A},\mathsf{x}_B \in {B}} d(\mathsf{x}_A,\mathsf{x}_B)
\end{equation}
where $d(\mathsf{x}_A,\mathsf{x}_B)$ is the distance between two objects
In \emph{complete-linkage} clustering (also called the diameter or maximum method), we consider the distance between cluster $A$ and cluster $B$ to be equal to the greatest distance from any member of one cluster to any member of the other cluster.
\begin{equation}
\label{eq:distance_complete_linkage}
d(A, B) = \max_{\mathsf{x}_A \in {A},\mathsf{x}_B \in {B}} d(\mathsf{x}_A,\mathsf{x}_B)
\end{equation}
In \emph{average-linkage} clustering, the distance between two clusters $A$ and $B$ is defined as the average distance from any streamline of cluster $A$ to any streamline of cluster $B$.
\begin{equation}
\label{eq:distance_average_linkage}
d(A, B) = avg_{\mathsf{x}_A \in {A},\mathsf{x}_B \in {B}} d(\mathsf{x}_A,\mathsf{x}_B)
\end{equation}

\subsection{Multiple scales for visualization}
\label{subsec:multiple_scales}
- the need of visualization the large dataset $\mathcal{X}$ at different scales, not display all the dataset \textbf{need to investigate}

The hierarchical tree $\mathcal{H}$ structures and presents dataset $\mathcal{X}$ at different levels of abstraction. A non-leaf cluster is composed of all its child clusters, while a leaf cluster contains only a single data item. The collection of all leaf-clusters presents exactly every data items $\mathsf{x}_i$ of $\mathcal{X}$, while the root is a cluster containing whole dataset $\mathcal{X}$ as one single node of the tree.

\textbf{Definition 2:} Each cluster $C_i$ (node) of the tree $\mathcal{H}$, let $s(C_i)$ be the \textbf{\textit{level of detail}} of that cluster. This measurement $s(C_i)$ satisfies the following criteria: if $C_i$ is an ancestor of $C_j$, then $s(C_i) \geq s(C_j)$. 

There are many properties of a cluster which could be used to measure $s(C_i)$. Among of them, two common use are the radius of a cluster (maximum distance between all pair samples of cluster $C_i$: $r_i = max_{\forall \mathsf{x}_a, \mathsf{x}_b
 \in C_i, \mathsf{x}_a \neq \mathsf{x}_b} \{d(\mathsf{x}_a,\mathsf{x}_b)\}$); and the hierarchical level of $C_i$ in the tree $\mathcal{H}$~\cite{yang2003interactive}.

\textbf{Definition 3:} The \textbf{\textit{range of scale}} of a hierarchical tree $\mathcal{H}$ is $[s_{min},s_{max}]$, where $s_{min} = \min_{\forall C_i \in \mathcal{H}}\{s(C_i)\}$, and $s_{max} = \max_{\forall C_i \in \mathcal{H}}\{s(C_i)\}$

Depending on which property is used to measure the level of detail $s_i$, the value of  $s_{max}$ and  $s_{min}$ would be different. In the case of using the hierarchical level, let $h$ be the heigh of the tree $\mathcal{H}$, then the level of detail of cluster $C_i$: $s(C_i) = \frac{height(C_i)}{h}$, where $heigh(C_i)$ is the heigh of the cluster $C_i$. The scale range is from $[0,1]$, where $s_{min} = 0$ corresponds to the leaf with zero heigh, to $s_{max} = 1$ is at the root of the tree $\mathcal{H}$. However, in the case of using cluster radius, there is no guarantee that $s_{min} = 0$ and $s_{max} = 1$.

\textbf{Definition 4:} \textbf{\textit{A cut $\mathfrak{L}$}} of a hierarchical tree $\mathcal{H}$ at a given scale $w \in [s_{min},s_{max}] $  is $\mathfrak{L}(w)$:
\begin{equation}
\label{equ:a_cut_l}
\mathfrak{L}(w) = \{C_i | (s(C_i) \leq w  \wedge s(parent(C_i)) > w)\}
\end{equation}
where $parent(C_i)$ is the direct parent node of the cluster $C_i$

Intuitively, the cut at $s_{min}$, $\mathfrak{L}(s_{min})$ is a set of all leaf clusters, while the $\mathfrak{L}(s_{max})$ is a single cluster representing the whole dataset $\mathcal{X}$. In general, $\mathfrak{L}(w)$ is a partion of $\mathcal{X}$, denoting a subset of the tree $\mathcal{H}$. $\mathfrak{L}(w)$ changes smoothly with the variance of the scale parameter $w$, which serves as the abstraction level of the dataset $\mathcal{X}$. It could be imagined that $\mathfrak{L}(w)$ is a cut across a vertically oriented hierarchical tree $\mathcal{H}$ that satisfies criteria: $\mathfrak{L}(w)$ intersecs each path of the tree $\mathcal{H}$, from the root to the leaf, only exactly at one point. The cutting point would depend on the value of parameter $w$. It should close to the root of the tree $\mathcal{H}$ when $w$ is high, and reversely. Moreover, the cut can be horizontal or unhorizontal (like zigzag) as long as for each path from the root to the leaf of the tree $\mathcal{H}$, there is only one crossing with $\mathfrak{L}(w)$. It is an open approach for cutting the tree comparing with the traditional one which only accepts the horizontal cut. 

\textbf{Definition 5:} Given the scale range $[s_{min},s_{max}]$ of a tree $\mathcal{H}$, the multiple scales representing for the tree $\mathcal{H}$ is a set of $k$ scale values from $[s_{min},s_{max}]$: $\mathsf{\textit{B}} = \{b_1, b_2, \ldots, b_k\}, b_i \in [s_{min},s_{max}], \forall i \in [1,k]$, where $k$ is the order of set $\mathsf{\textit{B}}$.

It is usually that $b_1 = s_{min}$ and $b_k = s_{max}$, where the whole elements of $\mathcal{X}$ are presented and only one virtual representation of $\mathcal{X}$ is showed. However, the value of $k$ is still an open question and totally depends on the application. In the next section, we will discuss about how to define this value and also how to select each $b_i$ from $[s_{min},s_{max}]$.

\subsection{How to choose the best scales}
\label{subsec:choosing_scales}

- how to choose the best scales 

How to choose the best scales to cut at, is driven from the paper~\cite{pons2011postprocessing}.

 
+ Measure the quality of a partition P:

- Silhouette Coefficient score: measure of how tightly grouped all the data in the cluster are. The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters. The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster. code [6], paper rousseeuw1987silhouette and web [7]

- Median partition: a good partition is the one which are most similar to all other partitions vega2011segmentation
% HKM (hierarchical k-means objective) & HRC (hierarchical ratio cut): kauchak03iterative

\subsection{Evaluation the best scales}
\label{subsec:evaluation_scales}

Given a hierarchical tree $\mathcal{H}$, and a multi-scale set representing for $\mathcal{H}$, $\mathsf{\textit{B}} = \{b_1, b_2, \ldots, b_k\}$. In this part, we focus on how to evaluate the quality of $\mathsf{\textit{B}}$
%scale $w \in [s_{min},s_{max}]$, the cut hierarchical tree $\mathcal{H}$ at $w$, $\mathfrak{L}(w)$, given by \ref{equ:a_cut_l} as: $\mathfrak{L}(w) = \{C_i | (s_i \leq w  \wedge s_{parent(C_i)} > w)\}$, and 

\textbf{\textit{Define the target criteria of cutting}}
Based on the real fact that, during the using of our software tool to do the segmentation of cortinal spinal tract, medical practitioners usually choose $~15 (\lambda_1)$ clusters from $~50 (\lambda_2)$representatives~\cite{prni2013-boi}, we propose a method to evaluate the represented multi-scale set $\mathsf{\textit{B}}$ based on the \textit{split factor} as following.

\textbf{Definition 6:} \textbf{\textit{Split factor $\xi$}} of \textit{a cluster} $C \in \mathcal{H}$ to a scale $w \in [s_{min},s_{max}]$ is $\xi(C,s)$
\begin{equation}
\xi(C,s) = card(P(C,s))
\end{equation}
where $P(C,s) = \{ C_j | C_j \in \mathcal{H} \wedge s(C_j) = w \wedge C_j \subseteq C \}$ 

\textbf{Definition 7:} \textbf{\textit{Split factor $\xi$}} of a \textit{set of cluster} $P=\{C_1, C_2,.., C_m\} \subseteq \mathcal{H}$ to a scale $s \in [s_{min},s_{max}]$ is $\xi(P,s)$
\begin{equation}
\xi(P,s) = \sum_{C_i \in P}\xi(C_i,s)
\end{equation}

Given a specific scale $w \in \mathsf{\textit{B}}$, draw uniformly at random $k_1$ clusters from the cut $\mathfrak{L}(w)$ of tree $\mathcal{H}$ at $w$, called $S_w$: $S_{w}=\{C_{s_1}, C_{s_2},.., C_{s_{k_1}}\}$, $C_{s_{i=1}^{k_1}} \in \mathfrak{L}(w)$.

\textbf{Definition 8:} The set of scale $\mathsf{\textit{B}} = \{b_1, b_2, \ldots, b_k\}$ is called the best scales for representation of the tree $\mathcal{H}$, if the following condition satisfies
\begin{equation}
\forall b \in [b_2,b_k]: \theta_2 - \Delta \leq \xi(S_b,b-1) \leq \theta_2 + \Delta 
\end{equation}

%\begin{algorithm}

\begin{algorithm}
\caption{Evaluate the cuts based on split factor}
\begin{algorithmic}Â 
\REQUIRE Hierarchical tree \mathcal{H}, the best cut set $\mathsf{\textit{B}} = \{b_1, b_2, \ldots, b_k\}$
%\ENSURE $y = x^n$
\STATE $i \leftarrow 1$
\REPEAT 
\STATE $w \leftarrow b_i$
\STATE $l \leftarrow b_{i+1}$

\STATE $P_l \leftarrow b_{i+1}$

\STATE $i \leftarrow i + 1$
\UNTIL{i+1 \geq k}
\IF{$n < 0$}
\STATE $X \leftarrow 1 / x$
\STATE $N \leftarrow -n$
\ELSE
\STATE $X \leftarrow x$
\STATE $N \leftarrow n$
\ENDIF
\WHILE{$N \neq 0$}
\IF{$N$ is even}
\STATE $X \leftarrow X \times X$
\STATE $N \leftarrow N / 2$
\ELSE[$N$ is odd]
\STATE $y \leftarrow y \times X$
\STATE $N \leftarrow N - 1$
\ENDIF
\ENDWHILE
\end{algorithmic}
\end{algorithm}
+ start from scale $l$, in current partition $P(l)$, draw $S=\{s_1,s_2,..,s_{k_1}\}$ of size $k_1$ uniformly at random

+ calculate split factor at scale $l$: $b(l) = card(P'(l+1))$, where $P'(l+1) = \{C_i, C_i \in P(l+1), \wedge C_i \in any s_j, with all j =1,..k1\}$

+ set current partition be $P'(l+1)$

+ recursively applying this procedure until we meet the last scale

+ the cut scales are good when all the branch factor $b(l)$ must be around $k_2$: $(k_2 - \theta) < b(l)< (k_2 + \theta)$, for all scale $l$
%\end{algorithm}

Let $P_i$ and $P_j$ represent for the level of abstraction $i$th and $j$th respectively. Then the relationship between clusters $C_{l}^{i} \in P_i$ and $C_{k}^{j} \in P_j$ can be presented as: 
\begin{equation}
\label{eq:parttition_ij}
%	\forall  P_i, P_j, i \geq j \longmapsto \exists C_{k}^{i}:  C_{k}^{i} \in P_j, k \in [1, \ldots, d_i]   
	\forall  P_i, P_j, i \geq j \mapsto [(C_{k}^{j} \subset C_{l}^{i}) \vee (C_{k}^{j} \cap  C_{l}^{i} = \emptyset)]    
\end{equation}
with $l \in [1, \ldots, d_i],k \in [1, \ldots, d_j]$. The most challenge is to design a clustering algorithm, based on a specific distance function, which is able to create a set of $m$-partition $\mathbb{P}$ of $\mathcal{T}$, which satisfies condition~\ref{eq:parttition_ij}. 
%This character would be effective in real time adapting to the responding of users. 
