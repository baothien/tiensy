@inproceedings{sculley2010web,
    abstract = {{We present two modifications to the popular k-means clustering algorithm to address the extreme requirements for latency, scalability, and sparsity encountered in user-facing web applications. First, we propose the use of mini-batch optimization for k-means clustering. This reduces computation cost by orders of magnitude compared to the classic batch algorithm while yielding significantly better solutions than online stochastic gradient descent. Second, we achieve sparsity with projected gradient descent, and give a fast ε-accurate projection onto the L1-ball. Source code is freely available: http://code.google.com/p/sofia-ml}},
    address = {New York, NY, USA},
    author = {Sculley, D.},
    booktitle = {Proceedings of the 19th international conference on World wide web},
    citeulike-article-id = {10163352},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1772862},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1772690.1772862},
    doi = {10.1145/1772690.1772862},
    isbn = {978-1-60558-799-8},
    keywords = {prni2013\_boi},
    location = {Raleigh, North Carolina, USA},
    pages = {1177--1178},
    posted-at = {2013-02-07 15:13:29},
    priority = {2},
    publisher = {ACM},
    series = {WWW '10},
    title = {{Web-scale k-means clustering}},
    url = {http://dx.doi.org/10.1145/1772690.1772862},
    year = {2010}
}

@inproceedings{olivetti2012approximation,
    author = {Olivetti, Emanuele and Nguyen, Thien B. and Garyfallidis, Eleftherios},
    booktitle = {IEEE International Workshop on Pattern Recognition in NeuroImaging},
    citeulike-article-id = {10745583},
    keywords = {frontiers2012, mlmi2012, prni2013\_boi},
    posted-at = {2012-06-05 22:29:00},
    priority = {2},
    publisher = {IEEE},
    title = {{The Approximation of the Dissimilarity Projection}},
    year = {2012}
}

@inproceedings{arthur2007kmeans,
    abstract = {{The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is Θ(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.}},
    address = {Philadelphia, PA, USA},
    author = {Arthur, David and Vassilvitskii, Sergei},
    booktitle = {Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms},
    citeulike-article-id = {3513166},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1283494},
    isbn = {978-0-898716-24-5},
    keywords = {dissimilarity\_approximation, prni2013\_boi},
    location = {New Orleans, Louisiana},
    pages = {1027--1035},
    posted-at = {2011-11-29 16:32:50},
    priority = {2},
    publisher = {Society for Industrial and Applied Mathematics},
    series = {SODA '07},
    title = {{k-means++: the advantages of careful seeding}},
    url = {http://portal.acm.org/citation.cfm?id=1283494},
    year = {2007}
}

